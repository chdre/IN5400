{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Train an image captioning network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Image Captioning is the process of generating a textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions*\"\n",
    "\n",
    "In this mandatory exercise, you are implementing an image captioning network. The network will consist of an encoder and a decoder. The encoder is a convolutional neural network, and the decoder is a recurrent neural network. Producing a reasonable textual description of an image is a hard task, however with the use of a CNN and an RNN, we can start to generate somewhat plausible descriptions. \n",
    "\n",
    "\n",
    "Links:\n",
    "- [Task1: Implementation](#Task1)\n",
    "- [Task2: Train the recurrent neural network](#Task2)\n",
    "- [Task3: Generate image captions](#Task3)\n",
    "\n",
    "\n",
    "Software version:\n",
    "- Python 3.7\n",
    "- Pytorch 1.4\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation format ###\n",
    "\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Note, you will often need to complete all previous steps in order to continue.\n",
    "\n",
    "In the implementation part, test functions allow you to check your code rapidly. When your code works, you can apply it on part 2 to train networks and part 3 to generate captions. Part 2, the training process, is slow and can run overnight on a typical laptop computer. \n",
    "\n",
    "There is no acceptance criterion in parts 2 and 3. Your task is, however, to train a couple of models and to generate plausible captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Delivery ###\n",
    "\n",
    "#### Deadline:  23:59 May 3, 2020\n",
    "\n",
    "You are to deliver the zip file created by \"collectSubmission.sh\".\n",
    "If you donâ€™t want to use the script, you can zip the files yourself. Please do not include the following folders to reduce the size of the zip file:\n",
    "    \n",
    "- data\n",
    "- storedModels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "\n",
    "All subtasks that you are to answer are found in this notebook. All implementation should be done in the file \"cocoSource.py\" found in folder \"/sourceFiles/\". The skeleton of the program is already implemented and contains things such as:\n",
    "- Importing data\n",
    "- Training framework\n",
    "- Saving and restoring models\n",
    "\n",
    "\n",
    "As mentioned, an image captioning network consists of an encoder and a decoder. Your task is to implement the decoder (RNN). The images have already been processed through a CNN. The CNN feature vectors are stored as pickle files together with the corresponding labels.\n",
    "\n",
    "\n",
    "During task 1, you will implement all required functionalities for training the image captioning network. In task 2, you will train the network and study how different RNN architectures influence the loss. You will generate image captions from images in the validation set in task 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset ###\n",
    "\n",
    "We will use a dataset called \"Common Object in Context\" (COCO) 2017. It has ~120,000 training and 5,000 validation images. Every image also includes ~5 captions.\n",
    "\n",
    "![](utils_images/bear.png)\n",
    "\n",
    "Captions:\n",
    "\n",
    "- A big burly grizzly bear is show with grass in the background.\n",
    "- The large brown bear has a black nose.\n",
    "- Closeup of a brown bear sitting in a grassy area.\n",
    "- A large bear that is sitting on grass. \n",
    "- A close up picture of a brown bear's face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Network architecture ###\n",
    "\n",
    "**Encoder**\n",
    "\n",
    "\n",
    "Convolutional neural networks have shown to be useful for extracting high-level features from images. We will use a pretrained Wide ResNet-101-2 network trained on ImageNet. ImageNet consists of 1,2 million images distributed over 1000 classes, and we hope the model have learned many general features. We are not interested in classifying the 1000 classes and will change the last fully connected layer with our own. We will use a tanh activation function to squeeze the feature values between -1 and 1, similar to the recurrent cells. \n",
    "\n",
    "**Decoder**\n",
    "\n",
    "To be able to convert the high-level features from the encoder to natural language, we will construct a recurrent neural network. The output of the encoder (CNN) will be passed as the initial state to the recurrent cells. \n",
    "The input to the recurrent neural network will be word embeddings, which we will learn. The goal is for the RNN to be able to predict the next word in the caption. \n",
    "\n",
    "**Loss function**\n",
    "\n",
    "The words will be considered as separate classes, and we shall use cross-entropy loss.\n",
    "\n",
    "**Training vs image caption generation**\n",
    "\n",
    "When we train the RNN, we will feed in the correct word (token) for every time step, see figure 2a. The words (tokens) are generally unknown, and while generating image captions, we can use our best estimate as input. For example, if the word \"brown\" has the highest probability after the softmax at timestep 2, we will feed in \"brown\" as input at timestep 3, see figure 2b.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/image_captioning_diagram_train_resnet.png\", width=2200>\n",
       "Figure 2a: The figure shows an example with 3 stacked recurrent cells in train mode.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_train_resnet.png\", width=2200>\n",
    "Figure 2a: The figure shows an example with 3 stacked recurrent cells in train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/image_captioning_diagram_test_resnet.png\", width=2200>\n",
       "Figure 2b: The figure shows an example with 3 stacked recurrent cells while generating image captions.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_test_resnet.png\", width=2200>\n",
    "Figure 2b: The figure shows an example with 3 stacked recurrent cells while generating image captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vocabulary ###\n",
    "\n",
    "\n",
    "To make good decisions in terms of the network architecture, we will need to know the statistics of the captions:\n",
    "- The number of words in the captions (sequence length) should be considered when choosing the truncated backpropagation length. \n",
    "- To save memory, it is normal to limit the vocabulary size/length. There will be a tradeoff between capturing all words and have a reasonably sized softmax layer. \n",
    "\n",
    "\n",
    "Note: The captions have been filtered such that all special characters have been removed. This also includes punctuation and commas. All characters have been changed to lower case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "**Vocabulary size**\n",
    "\n",
    "To make good predictions, we need the model to have the ability to predict frequent words. Figure 3 shows a sorted histogram of the word count for different words. We can see that the majority of the words are within the 2000 most frequent words. Figure 4 shows the percentage of the words accounted for as a function of the vocabulary size.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
       "*Figure 3*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
    "*Figure 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
       "*Figure 4*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
    "*Figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Truncated backpropagation length**\n",
    "\n",
    "The sequence length of the recurrent neural network should be able to capture the time(step) dependencies within the captions. Figure 5 shows a histogram of the caption counts as a function of the caption (sequence) length. Figure 6 shows the percentage of all captions with shorter or equal caption length as a function of caption (sequence) length. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
       "*Figure 5*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
    "*Figure 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
       "*Figure 6*\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
    "*Figure 6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Common words and their tokens**\n",
    "\n",
    "\n",
    "Every word is associated with a token. The words are sorted such that the most frequent words have lower token values. In the table below, you find the six most common words in the dataset. The three first words/tokens have a special purpose:\n",
    "- \"ssss\": Start token, the first word \n",
    "- \"eeee\": Indicates the end of a caption.\n",
    "- \"UNK\": As the vocabulary size often is smaller than the number of words in the dataset, we change all \"unknown\" words to the word \"UNK\"\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"utils_images/vocabulary.png\", width=800>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/vocabulary.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Word embedding  ###\n",
    "\n",
    "\n",
    "The input to the first rnn layer is the embedded version of the input words. We will define a word embedding matrix with shape [vocabulary_size, embedding_size)]. A single word embedding will be a row vector with length [embedding_size]. The tokens are used to select the correct row within the word embedding matrix. For example, the word \"a\" will have values in the third row in the embedding matrix (python notation: the first row is row zero).\n",
    "\n",
    "\n",
    "a_emb = wordEmbedding[a_token, :]\n",
    "\n",
    "\n",
    "The word embedding matrix will be initialized with random values, and they will be updated as part of the training process. The goal is that similar words get similar vector representations within the embedding matrix. This will not be part of the assignment, but as a check, the vector representations could be embedded using e.g., t-SNE for plotting in 2D/3D space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Data preparation  ###\n",
    "\n",
    "Before starting to work on implementing the network architecture, you should check if you need to download and process data:\n",
    "\n",
    "- **UIO**: If you work on an IFI computer or one of the ML servers, we have already processed and made the data available for you. The paths are given in the notebook.\n",
    "\n",
    "- **Personal computer**: If you plan to work on the assignment on your personal computer, you will need to download the data yourself. The notebook **data_preparation_download_onedrive** will help you with the data preparation.\n",
    "\n",
    "\n",
    "The processed data are pickle files holding the following information for every image:\n",
    "- Path to the jpg image\n",
    "- Captions\n",
    "- Captions given as tokens\n",
    "- Image features produced by the Wide ResNet-101-2 network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Task 1: Implementation #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarting the notebook after each subtask can be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that~ the notebook will reload external python modules;\n",
    "# see http://stackoverflo.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1a** \n",
    "\n",
    "Your are to implement a vanilla RNN class in the function RNNCell in file sourceFiles/cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated below.\n",
    "\n",
    "Vanilla RNN:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= tanh(x_tW^{hx} + h_{t-1}W^{hh} + b) \\\\\n",
    "\\\\\n",
    "h_t &= tanh([x_t,h_{t-1}]W + b)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"weight\" matrix has correct shape\n",
      "The \"bias\" matrix has correct shape\n",
      "The \"weight\" matrix has correct values\n",
      "The \"bias\" matrix has correct values\n",
      "The \"forward\" function is implemented correctly\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 256\n",
    "batch_size = 128\n",
    "\n",
    "x          = torch.tensor(np.random.rand(batch_size, inputSize).astype(np.float32))\n",
    "state_old  = torch.tensor(np.random.rand(batch_size, hidden_state_sizes).astype(np.float32))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.RNNCell(hidden_state_sizes, inputSize)\n",
    "cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.RNNcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1b** \n",
    "\n",
    "You are to implement a Gated recurrent units (GRU) class in the function GRUCell in cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatenate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated in the equations below, where you are to implement the equations in the column to the right.\n",
    "\n",
    "GRU:\n",
    "\n",
    "\\begin{align}\n",
    "&Update \\: gate: \\qquad &\\Gamma^u=\\sigma(x_tW^{u} + h_{t-1}U^{u} + b^u) \\qquad \\rightarrow \\qquad &\\Gamma^u=\\sigma([x_t, h_{t-1}]W^{u} + b^u) \\\\\n",
    "&Reset \\: gate: \\qquad &\\Gamma^r=\\sigma(x_tW^{r} + h_{t-1}U^{r} + b^r) \\qquad \\rightarrow \\qquad &\\Gamma^r=\\sigma([x_t, h_{t-1}]W^{r} + b^r) \\\\\n",
    "&Candidate \\:cell: &\\tilde{h_t} = tanh([x_tW + (\\Gamma^r \\circ h_{t-1})U + b) \\qquad \\rightarrow \\qquad &\\tilde{h_t} = tanh([x_t, (\\Gamma^r \\circ h_{t-1})] W + b) \\\\\n",
    "&Final\\: cell:    &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t} \\qquad \\rightarrow \\qquad &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"weight_r\" matrix has correct shape\n",
      "The \"weight_u\" matrix has correct shape\n",
      "The \"weight\" matrix has correct shape\n",
      "The \"bias_r\" matrix has correct shape\n",
      "The \"bias_u\" matrix has correct shape\n",
      "The \"bias\" matrix has correct shape\n",
      "The \"weight_r\" matrix has correct values\n",
      "The \"weight_u\" matrix has correct values\n",
      "The \"weight\" matrix has correct values\n",
      "The \"bias_r\" matrix has correct values\n",
      "The \"bias_u\" matrix has correct values\n",
      "The \"bias\" matrix has correct values\n",
      "The \"forward\" function is implemented correctly\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 648\n",
    "batch_size = 128\n",
    "\n",
    "x          = torch.tensor(np.random.rand(batch_size, inputSize).astype(np.float32))\n",
    "state_old  = torch.tensor(np.random.rand(batch_size, hidden_state_sizes).astype(np.float32))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.GRUCell(hidden_state_sizes, inputSize)\n",
    "cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.GRUcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1c** \n",
    "\n",
    "Your task is to implement \"loss_fn\" in cocoSource.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumLoss has correct value\n",
      "meanLoss has correct value\n"
     ]
    }
   ],
   "source": [
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "weight_dir = 'unit_tests/loss_fn_tensors.pt'\n",
    "checkpoint = torch.load(weight_dir)\n",
    "    \n",
    "logits     = checkpoint['logits']\n",
    "yTokens    = checkpoint['yTokens']\n",
    "yWeights   = checkpoint['yWeights']\n",
    "sumLoss, meanLoss = cocoSource.loss_fn(logits, yTokens, yWeights)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.loss_fn_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1d** \n",
    "\n",
    "You may want to have a look at \"utils/trainer.py\" and \"utils/model.py\" to understand the flow of the script.\n",
    "Your task is to implement the  RNN class in cocoSource.py. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- For is_train = True --------\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n",
      "-------- For is_train = False --------\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'sourceFiles.cocoSource.RNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:593: SourceChangeWarning: source code of class 'sourceFiles.cocoSource.GRUCell' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/RNN_tensors_is_train_True.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "\n",
    "outputLayer   = checkpoint['outputLayer']\n",
    "Embedding     = checkpoint['Embedding']\n",
    "xTokens       = checkpoint['xTokens']\n",
    "initial_hidden_state  = checkpoint['initial_hidden_state']\n",
    "input_size            = checkpoint['input_size']\n",
    "hidden_state_size     = checkpoint['hidden_state_size']\n",
    "num_rnn_layers        = checkpoint['num_rnn_layers']\n",
    "cell_type             = checkpoint['cell_type']\n",
    "is_train              = checkpoint['is_train']\n",
    "# You should implement this function\n",
    "myRNN = cocoSource.RNN(input_size, hidden_state_size, num_rnn_layers, cell_type)\n",
    "logits, current_state = myRNN(xTokens, initial_hidden_state, outputLayer, Embedding, is_train)\n",
    "\n",
    "#Check implementation\n",
    "is_train = True\n",
    "unit_tests.RNN_test(is_train)\n",
    "\n",
    "is_train = False\n",
    "unit_tests.RNN_test(is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1e** \n",
    "\n",
    "You should now implement the imageCaptionModel class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- With \"current_hidden_state\"==None -----\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n",
      " \n",
      "---- With \"current_hidden_state\"!=None -----\n",
      "\"logits\" has correct shape\n",
      "\"logits\" has correct values\n",
      "\"current_hidden_state\" has correct shape\n",
      "\"current_hidden_state\" has correct values\n"
     ]
    }
   ],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/imageCaptionModel_tensors.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "config                            = checkpoint['config']\n",
    "config['number_of_cnn_features']  = checkpoint['vgg_fc7_features'].shape[1]\n",
    "cnn_features                      = checkpoint['vgg_fc7_features']\n",
    "xTokens                           = checkpoint['xTokens']\n",
    "is_train                          = checkpoint['is_train']\n",
    "\n",
    "myImageCaptionModel = cocoSource.imageCaptionModel(config)\n",
    "logits, current_hidden_state = myImageCaptionModel(cnn_features, xTokens,  is_train)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.imageCaptionModel_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "# Task 2: Train the image captioning network #\n",
    "\n",
    "\n",
    "Congratulations, you are done with the implementation part. The default \"config\" dictionary gives you a decent set of hyperparameters. However, don't expect the generated captions to be perfect. Depending on your computing power, the training can take a long time. We do not expect you to optimize hyperparameters. \n",
    "\n",
    "Running this cell will likely take overnight if you run on a CPU. \n",
    "If you have issues with the computing time running this cell, it will not be mandatory to pass the exercise. It is however fun to see how well your network can do in predicting captions. \n",
    "\n",
    "In the folder \"loss_images/\", you can find the training curves as PNGs. The name of the PNGs is defined by the key \"modelName\" within the \"modelParam\" dictionary. \n",
    "\n",
    "The validation loss is calculated for every epoch. If the validation loss is lower, then the lowest validation loss so far, we store the network weight. These weights are restored in task 3, where you are to generate captions.\n",
    "\n",
    "If you have an Nvidia GPU, you can set the \"use_cuda\" flag to True to utilize your GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\utils\\plotter.py:12: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  self.fig.show()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c901f2507b24faa9268e7218bdddd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=925), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for sequence element 1 in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-48ccaec09397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# here you train your model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelParam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveRestorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\utils\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\utils\\trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(self, mode, model, is_train, cur_epoch)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcnn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cnn_features'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                     \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_hidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTokens\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_hidden_state_Ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTokens\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\sourceFiles\\cocoSource.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, cnn_features, xTokens, is_train, current_hidden_state)\u001b[0m\n\u001b[0;32m     73\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputLayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                                             is_train=is_train)\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\sourceFiles\\cocoSource.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xTokens, initial_hidden_state, outputLayer, Embedding, is_train)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rnn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# All layers inside each column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# New state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                     \u001b[0minitial_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m   \u001b[1;31m# Updating state for next layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\IN5400\\Oblig2\\In5400_2020_oblig2_assignment\\sourceFiles\\cocoSource.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, state_old)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# TODO:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mconcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_old\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mu_gate\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_u\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_u\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for sequence element 1 in sequence argument at position #1 'tensors'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = '/projects/in5400/oblig2/coco/'\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "#data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "#train\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': False, # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 2,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 20,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': False\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.001},   # learning rate to the optimizer\n",
    "        'weight_decay': 0.00001,         # weight_decay value\n",
    "        'number_of_cnn_features': 2048,  # Fixed, do not change\n",
    "        'embedding_size': 300,           # word embedding size\n",
    "        'vocabulary_size': 10000,        # number of different words\n",
    "        'truncated_backprop_length': 25, #\n",
    "        'hidden_state_sizes': 256,       # \n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'GRU'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "\n",
    "# create an instance of the model you want\n",
    "model = Model(config, modelParam)\n",
    "\n",
    "# create an instacne of the saver and resoterer class\n",
    "saveRestorer = SaverRestorer(config, modelParam)\n",
    "model        = saveRestorer.restore(model)\n",
    "\n",
    "# create your data generator\n",
    "dataLoader = DataLoaderWrapper(config, modelParam)\n",
    "\n",
    "# here you train your model\n",
    "trainer = Trainer(model, modelParam, config, dataLoader, saveRestorer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task3'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 3: Generate image captions on the validation data set#\n",
    "\n",
    "Note: The config dictionary needs to be identical to the one used for training the model. Use modelParam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = /projects/in5400/oblig2/coco/\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "#data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': False,  # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 2,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 15,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': True\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.001},   # learning rate to the optimizer\n",
    "        'weight_decay': 0.00001,         # weight_decay value\n",
    "        'number_of_cnn_features': 2048,  # Fixed, do not change\n",
    "        'embedding_size': 300,           # word embedding size\n",
    "        'vocabulary_size': 10000,        # number of different words\n",
    "        'truncated_backprop_length': 25,\n",
    "        'hidden_state_sizes': 256,       #\n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'GRU'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "if modelParam['inference'] == True:\n",
    "    modelParam['batch_size'] = 1\n",
    "    modelParam['modeSetups'] = [['val', False]]\n",
    "    modelParam['restoreModelBest'] = 1\n",
    "\n",
    "# create an instance of the model you want\n",
    "model = Model(config, modelParam)\n",
    "\n",
    "# create an instacne of the saver and resoterer class\n",
    "saveRestorer = SaverRestorer(config, modelParam)\n",
    "model        = saveRestorer.restore(model)\n",
    "\n",
    "# create your data generator\n",
    "dataLoader = DataLoaderWrapper(config, modelParam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to generate captions. Run it several times to see results for different images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotImagesAndCaptions\n",
    "plotImagesAndCaptions(model, modelParam, config, dataLoader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
