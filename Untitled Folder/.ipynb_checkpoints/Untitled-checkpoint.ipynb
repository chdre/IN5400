{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksamen IN5400\n",
    "Kandidatnr. 15138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument $z_k^{[l]}$ of the activation function $g(z)$ for node k and layer l is\n",
    "\\begin{equation}\n",
    "z_k^{[l]} = \\sum_{j=1}^{n^{l-1}} W_{jk}^{[l]} a_j^{[l-1]} + b_k^{[l]},\n",
    "\\end{equation}\n",
    "where the sum runs over nodes in layer $l-1$ (where 0 corresponds to input).\n",
    "\n",
    "From the input to the hidden layer, first neuron:\n",
    "\n",
    "$$z_1^{[1]} = \\sum_{j=1}^{n^0}W_{j1}^{[1]}a_j^{[0]} = \\sum_{j=1}^{3}W_{j1}^{[1]}a_j^{[0]},$$\n",
    "\n",
    "where we have omitted the bias term since it is zero. Also $a_j^{[0]} = x_j$.\n",
    "\n",
    "$$z_1^{[1]} = W_{11}^{[1]}x_1 + W_{21}^{[1]}x_2 + W_{31}^{[1]}x_3 = 1\\times 2 + 1\\times 1 + 2 \\times 3 = 9$$\n",
    "\n",
    "Second neuron:\n",
    "\\begin{align}\n",
    "z_2^{[1]} &= \\sum_{j=1}^{n^0}W_{j2}^{[1]}a_j^{[0]} \\\\\n",
    " &= W_{12}^{[1]}x_1 + W_{22}^{[1]}x_2 + W_{32}x_3 \\\\\n",
    " &= 2\\times 2 + 3\\times 1 + 1\\times 3 \\\\\n",
    " &= 10.\n",
    "\\end{align}\n",
    "\n",
    "Which gives $a_1 = sin(9) = 0.41211849$ and $a_2 = sin(10) = -0.54402111$.\n",
    "\n",
    "For the hidden layer to the output:\n",
    "\\begin{align}\n",
    "z_1^{[2]} &= \\sum_{j=1}^{n^1}W_{j1}^{[2]}a_j^{[2]} = \\sum_{j=1}^{2}W_{j1}^{[2]}a_j^{[2]} \\\\\n",
    "&= W_{11}^{[2]}a_1^{[1]} + W_{21}^{[2]}a_2^{[1]} \\\\\n",
    "&= 3\\times 0.41211849 + 4 \\times -0.54402111 \\\\\n",
    "&= 1.23635547 - 2.17608444 \\\\\n",
    "&= -0.93972897\n",
    "\\end{align}\n",
    "\n",
    "The final output is then $\\hat{y} = \\sigma(-0.93972897) = 0.28095509$, which corresponds to classification of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true output is $y = 1$ and we have $\\hat{y} = 0.28095509$ for a single pass through the NN.\n",
    "\n",
    "The logistic cost function for the case of a single sample $y$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "L(W, b) = \\hat{y}^y(1 - \\hat{y}^{1 - y},\n",
    "\\end{equation}\n",
    "\n",
    "where we have omitted the sum $\\sum_{i=1}^m$ and the normalization factor $1/m$, where m ($=1$) is the number of samples.\n",
    "\n",
    "Taking the logarithm we get\n",
    "\n",
    "\\begin{equation}\n",
    "l(W, b) = \\log(L(W,b)) = y\\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}),\n",
    "\\end{equation}\n",
    "\n",
    "which is easier to maximize.\n",
    "\n",
    "The updated weight are\n",
    "\n",
    "\\begin{align}\n",
    "W_{11}^{[2]} &= W_{11}^{[2]} - \\lambda (y - \\hat{y})a_1 \\\\\n",
    "&= 3 - 0.2 (1 - 0.28095509)\\times 0.41211849\\\\\n",
    "&= 3 - 0.143808982\\times 0.41211849 \\\\\n",
    "&= 2.940733,\n",
    "\\end{align}\n",
    "\n",
    "where we have inserted $x_1$ for the input corresponding to weight $W_{11}^{[2]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sine and, the derivative, cosine are non-monotonic, i.e. if given inputs x and y where $x \\geq y$ or $x < y$ sine and cosine does not have the property that $sin(x) \\geq sin(y)$ $\\forall x \\geq y$, same goes for $x < y$ and cosine. Training the model when the activation function is non-monotonic may result in varying loss, and therefore effect the stability of the gradient update as there may be multiple local minima, furthermore it may take longer time to train the model. For tasks that are periodic it may be beneficial to use sine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully connected NN requires weights for every single pixel in the image for a single neuron. If the input image is $64x64x3$, a 64x64 color image, it would require $64\\times64\\times3=12288$ weights for a single neuron. Increasing the image size rapidly increases the amount of weights needed, and further we most likely want more than one neuron, add to the total amount of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical receptive field covers the entirety of the (colored blue in the image below) input with which the neuron is connected with (colored red). We see that the outer values from the input has less connections to the neuron (red). The effective receptive field covers the same input size (blue), but as we increase the number of layers the outer values become less import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a CNN to detect long range dependencies we have to use networks with many layers to increase the effective receptive field. (Lindsley et al. 2018). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>\n",
    "<cite data-cite=\"3514957/JSXBTV68\"></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "3514957/JSXBTV68": {
     "URL": "http://papers.nips.cc/paper/7300-learning-long-range-spatial-dependencies-with-horizontal-gated-recurrent-units.pdf",
     "accessed": {
      "day": 3,
      "month": 6,
      "year": 2020
     },
     "author": [
      {
       "family": "Linsley",
       "given": "Drew"
      },
      {
       "family": "Kim",
       "given": "Junkyung"
      },
      {
       "family": "Veerabadran",
       "given": "Vijay"
      },
      {
       "family": "Windolf",
       "given": "Charles"
      },
      {
       "family": "Serre",
       "given": "Thomas"
      }
     ],
     "container-title": "Advances in Neural Information Processing Systems 31",
     "editor": [
      {
       "family": "Bengio",
       "given": "S."
      },
      {
       "family": "Wallach",
       "given": "H."
      },
      {
       "family": "Larochelle",
       "given": "H."
      },
      {
       "family": "Grauman",
       "given": "K."
      },
      {
       "family": "Cesa-Bianchi",
       "given": "N."
      },
      {
       "family": "Garnett",
       "given": "R."
      }
     ],
     "id": "3514957/JSXBTV68",
     "issued": {
      "year": 2018
     },
     "page": "152â€“164",
     "page-first": "152",
     "publisher": "Curran Associates, Inc.",
     "title": "Learning long-range spatial dependencies with horizontal gated recurrent units",
     "type": "chapter"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
